SRILM(Stanford Research Institute Language Modeling Toolkit)


PageRank 
监督学习又可以分为两类: 判别模型(Discriminative model)和生成模型(Generative model) 

监督式学习的例子有：回归、决策树、随机森林、K – 近邻算法、逻辑回归等
非监督式学习的例子有：关联算法和 K – 均值算法。

HTK - Hidden Markov Model Toolkit

 机器学习任务主要包含两大类。第一种属于模式识别范畴，认为所有的样本之间相互独立，可以直接将每个样本的特征输入机器学习模型进行训练，如逻辑回归，SVM，随机森林，稀疏自编码等模型；另一种属于序列任务，需要考虑样本之间的某种关联，如预测天气状况，很明显，今天的天气（一个样本）和昨天的天气（另一个样本）有关，这样的任务如果仅仅使用模式识别，将忽略样本之间的联系，达不到理想的效果，一般这种问题的解决办法是使用固定窗口，即将某一个连续的范围内的所有样例共同训练，从而提高准确率，另一种办法是使用带有时序的模型，如HMM，RNN，LSTM等。

HMM详解
http://blog.csdn.net/daringpig/article/details/8072794

假设观测结果有M个，隐藏状态有N个，那么转移矩阵大小为N*N,混淆矩阵的 大小为N*M。隐藏状态的数目和观测结果的数目是可以不同的。
混淆矩阵的列是隐藏状态，行是观测结果，所以矩阵的每一行概率的和都是1。
初始向量，定义系统初始化时每个状态的概率。

语音识别的过程：1）语音分成frame，每个frame提取特征声学特征mfcc/fbank/... 2） 每个frame的特征跑GMM，得到每个frame属于每个音素的概率 

https://xycui.wordpress.com/2005/06/01/%e9%9a%90%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e6%a8%a1%e5%9e%8bhmm%e8%87%aa%e5%ad%a6-%ef%bc%883%ef%bc%89/

语音识别相关资源：
http://blog.csdn.net/xiaoding133/article/details/8842945

语音解码，输出路径和输出序列是多对一关系

机器学习
http://lib.csdn.net/machinelearning/node/25

总结 HMM可以解决的三类问题
评估:根据已知的HMM找出一个观察序列的概率。
解码:根据观察序列找到最有可能出现的隐状态序列
学习:从观察序列中得出HMM

https://zhuanlan.zhihu.com/p/26162237
EM算法也被称为上帝的算法，它常用于含有隐变量的概率模型中的极大似然估计。EM算法称为期望最大化算法（Expectation Maximization Algorithm），由名字就可以看出，它的算法思想很简单，基本来说由两步组成
    E步：求期望（Expectation）
    M步：求极大（Maximization）
可以给出EM算法的步骤：
    选择合适的初始参数，开始迭代（EM算法是初值敏感的）
    E步：记theta为第i次参数的估计值，计算第i+1次的Q函数的值
    M步：求使Q极大化的参数theta，确定新的参数估计值theta_{i+1}
    重复（2），（3）直到收敛



MLP->RNN->LSTM。还有LSTM本身的发展路线（97年LSTM到forget gate到peephole再到CTC ）



NN deep learning
http://blog.csdn.net/dark_scope/article/details/9421061

英文音节　元音２０个　辅音２８个
https://www.zybang.com/question/07a0ed53f2c33a357442044f7732e2d3.html

梯度下降法 随机梯度下降

Gradient Boosting 和 AdaBoost 算法

欠拟合、过拟合及其解决方法             http://lib.csdn.net/article/machinelearning/50713
	欠拟合:
		添加其他特征项，有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决
		添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。
		减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。
	过拟合:
		重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。
		增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。
		采用正则化方法。正则化方法包括L0正则、L1正则和L2正则，而正则一般是在目标函数之后加上对应范数。


condition number就是拿来衡量ill-condition系统的可信度的; condition number衡量的是输入发生微小变化的时候，输出会发生多大的变化。也就是系统对微小变化的敏感度。
condition number值小的就是well-conditioned的，大的就是ill-conditioned的。

排序(Ranking)一直是信息检索的核心研究问题，有大量的成熟的方法，主要可以分为以下两类：相关度排序模型和重要性排序模型。
L0正则：模型参数中非零参数的个数
L1正则：所有特征的绝对值之和
L2正则：所有特征的平方和开根

各个模型的Loss function，牛顿学习法、SGD如何训练

LR、RF、GBDT(Gradient Boosting Decision Tree) ，分析它们的优缺点，是否写过它们的分布式代码
SVD、SVD++
线性加权、bagging、boosting、cascade　等模型融合方式
推荐系统的冷启动问题如何解决
分类（3类）：
1）用户冷启动：如何给新用户做个性化推荐
2）物品冷启动：如何将新物品推荐给可能对其感兴趣的用户。在新闻网站等时效性很强的网站中非常重要。
3）系统冷启动：如何在一个新开发的网站上设计个性化推荐，从而在网站刚发布时就让用户体验到个性化推荐服务。没有用户，只有一些物品信息。


利用用户注册信息解决冷启动问题
	人口统计学信息。如年龄、性别、职业、学历等
	用户兴趣描述
	从其它网站导入的用户站外行为数据。如链接豆瓣、新浪等。

选择合适的物品启动用户的兴趣解决冷启动
	通过让用户对物品进行评分来收集用户兴趣　
	需评分的物品应较热门、有代表性和区分性、多样性

利用物品的内容信息解决冷启动


A/B Test以及A/B Test结果的置信度
A/B测试的核心在于：在对同一个要素有两种版本(A/B)并且有度量哪个更成功的指标的情况下，将A/B两个版本都同时地做实验，然后根据度量结果来决定哪个版本更好，从而决定在真正去使用哪个版本。
特征工程经验
是否了解mutual infomation、chi-square、LR前后向、树模型等特征选择方式

fork调用的一个奇妙之处就是它仅仅被调用一次，却能够返回两次，它可能有三种不同的返回值：
    1）在父进程中，fork返回新创建子进程的进程ID；
    2）在子进程中，fork返回0；
    3）如果出现错误，fork返回一个负值；

http://www.cnblogs.com/feisky/archive/2013/01/09/2852663.html
http://coolshell.cn/articles/7829.html
http://coolshell.cn/articles/7965.html

UTF-8是一种变长字节编码方式。对于某一个字符的UTF-8编码，如果只有一个字节则其最高二进制位为0；如果是多字节，其第一个字节从最高位开始，连续的二进制位值为1的个数决定了其编码的位数，其余各字节均以10开头。UTF-8最多可用到6个字节。
如表：
1字节 0xxxxxxx
2字节 110xxxxx 10xxxxxx
3字节 1110xxxx 10xxxxxx 10xxxxxx
4字节 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
5字节 111110xx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx
6字节 1111110x 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 

http://blog.csdn.net/baixiaoshi/article/details/40786503

0-127编码 ANSI 的”Ascii”编码（American Standard Code for Information Interchange，美国信息互换标准代码）。当时世界上所有的计算机都用同样的ASCII方案来保存英文文字。

一个小于127的字符的意义与原来相同，但两个大于127的字符连在一起时，就表示一个汉字，前面的一个字节（他称之为高字节）从0xA1用到 0xF7，后面一个字节（低字节）从0xA1到0xFE，这样我们就可以组合出大约7000多个简体汉字了。在这些编码里，我们还把数学符号、罗马希腊的字母、日文的假名们都编进去了，连在 ASCII 里本来就有的数字、标点、字母都统统重新编了两个字节长的编码，这就是常说的"全角"字符，而原来在127号以下的那些就叫"半角"字符了。这种汉字方案叫做"GB2312"。GB2312 是对 ASCII 的中文扩展。

后来不再要求低字节一定是127号之后的内码，只要第一个字节是大于127就固定表示这是一个汉字的开始，不管后面跟的是不是扩展字符集里的内容。结果扩展之后的编码方案被称为 GBK 标准，GBK 包括了 GB2312 的所有内容，同时又增加了近20000个新的汉字（包括繁体字）和符号。

ISO （国际标谁化组织）采用的方法：废了所有的地区性编码方案，重新搞一个包括了地球上所有文化、所有字母和符号的编码！他们打算叫它”Universal Multiple-Octet Coded Character Set”，简称 UCS, 俗称 “UNICODE”。

UNICODE 在网络传输中，出现了两个标准 UTF-8 和 UTF-16，分别每次传输 8个位和 16个位。

在UNICODE 中，一个字符就是两个字节。一个汉字算两个英文字符的时代已经快过去了。
Unicode 第一个字节在前，就是”大头方式“（Big endian），第二个字节在前就是”小头方式“（Little endian）。


汉字占字节数和文本编码有关
GB2312:两个字节
Unicode:两个字节
UTF-8:可变，一般为3个字节

GB2312在Windows上用的多

UTF-8在其他系统上（Linux这些）比较常见



